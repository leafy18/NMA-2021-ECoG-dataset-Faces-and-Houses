# -*- coding: utf-8 -*-
"""Final_ECoG_faceshouses Analysis(Spry Ugusius).ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1J1d-TBnFZhcADMB5UOWboG90L3eon0Is

## Loading of Miller ECoG data of faces/houses (+ noise)

includes some visualizations
"""

#@title Data retrieval
import os, requests

fname = 'faceshouses.npz'
url = "https://osf.io/argh7/download"

if not os.path.isfile(fname):
  try:
    r = requests.get(url)
  except requests.ConnectionError:
    print("!!! Failed to download data !!!")
  else:
    if r.status_code != requests.codes.ok:
      print("!!! Failed to download data !!!")
    else:
      with open(fname, "wb") as fid:
        fid.write(r.content)

#@title Install packages, import matplotlib and set defaults
# install packages to visualize brains and electrode locations
!pip install nilearn --quiet
!pip install nimare --quiet

from matplotlib import rcParams 
from matplotlib import pyplot as plt
rcParams['figure.figsize'] = [20, 4]
rcParams['font.size'] =15
rcParams['axes.spines.top'] = False
rcParams['axes.spines.right'] = False
rcParams['figure.autolayout'] = True

#@title Data loading
import numpy as np

alldat = np.load(fname, allow_pickle=True)['dat']

# select just one of the recordings here. 
dat1 = alldat[1][0]
dat2 = alldat[1][1]

print(dat1.keys())
print(dat2.keys())

dat2 = alldat[0][1]
print(dat2['key_press'])

"""# Dataset info #

This is one of multiple ECoG datasets from Miller 2019, recorded in a clinical settings with a variety of tasks. We plan to curate a few more before NMA starts. Raw data here:

https://exhibits.stanford.edu/data/catalog/zk881ps0522

`alldat` contains 7 subjects each with two sessions `dat1` and `dat2`, and was originally used in these publications: 

*Miller, Kai J., et al. "Face percept formation in human ventral temporal cortex." Journal of neurophysiology 118.5 (2017): 2614-2627.*

*Miller, Kai J., et al. "The physiology of perception in human temporal lobe is specialized for contextual novelty." Journal of neurophysiology 114.1 (2015): 256-263.*

*Miller, Kai J., et al. "Spontaneous decoding of the timing and content of human object perception from cortical surface recordings reveals complementary information in the event-related potential and broadband spectral change." PLoS computational biology 12.1 (2016): e1004660.*

*Miller, Kai J., et al. "The physiology of perception in human temporal lobe is specialized for contextual novelty." Journal of neurophysiology 114.1 (2015): 256-263.*

*Miller, Kai J., et al. "Spontaneous decoding of the timing and content of human object perception from cortical surface recordings reveals complementary information in the event-related potential and broadband spectral change." PLoS computational biology 12.1 (2016): e1004660.*

In this task, subjects in a clinical settings (with ECoG implants) are passively shown faces and house during the first experiment (`dat1`). Then in the second experiment in the same subjects (`dat2`), noise is added to face and houses images and the subject has to detect the faces by pressing a key. Two of the subjects don't have keypresses. 

Sample rate is always 1000Hz, and the ECoG data has been notch-filtered at 60, 120, 180, 240 and 250Hz, followed by z-scoring across time and conversion to float16 to minimize size. 

Experiment 1: 
* `dat1['V']`: continuous voltage data (time by channels)
* `dat1['srate']`: acquisition rate (1000 Hz). All stimulus times are in units of this.  
* `dat1['t_on']`: time of stimulus onset in data samples
* `dat1['t_off']`: time of stimulus offset, always 400 samples after `t_on`
* `dat1['stim_id`]: identity of stimulus from 1-100, with 1-50 being houses and 51-100 being faces
* `dat1['locs`]: 3D electrode positions on the brain surface

Experiment 2: 
* `dat2['V`]: continuous voltage data (time by channels)
* `dat2['srate']`: acquisition rate (1000 Hz). All stimulus times are in units of this.  
* `dat2['t_on']`: time of stimulus onset in data samples
* `dat2['t_off']`: time of stimulus offset, always 1000 samples after `t_on`, with no inter-stimulus interval
* `dat2['stim_id`]: identity of stimulus from 1-600 (not really useful, since we don't know which ones are the same house/face)
* `dat2['stim_cat']`: stimulus category (1 = house, 2 = face)
* `dat2['stim_noise']`: percent noise from 0 to 100
* `dat2['key_press']`: when the subject thought the image was a face
* `dat2['categories']`: categories legend (1 = house, 2 = face)
* `dat2['locs`]: 3D electrode positions on the brain surface

"""

from nilearn import plotting  
from nimare import utils

plt.figure(figsize=(8,8))
locs = dat1['locs']
view = plotting.view_markers(utils.tal2mni(locs), marker_labels  = ['%d'%k for k in np.arange(locs.shape[0])], marker_color = 'purple', marker_size=5)
view

# quick way to get broadband power in time-varying windows
from scipy import signal

V = dat1['V'].astype('float32')

b, a = signal.butter(3, [50], btype = 'high', fs=1000)
V = signal.filtfilt(b,a,V,0)
V = np.abs(V)**2
b, a = signal.butter(3, [10], btype = 'low', fs=1000)
V = signal.filtfilt(b,a,V,0)

V = V/V.mean(0)

# average the broadband power across all face stimuli and across all house stimuli

nt, nchan = V.shape
nstim = len(dat1['t_on'])

trange = np.arange(-200, 400)
ts = dat1['t_on'][:,np.newaxis] + trange
V_epochs = np.reshape(V[ts, :], (nstim, 600, nchan))

V_house = (V_epochs[dat1['stim_id']<=50]).mean(0)
V_face  = (V_epochs[dat1['stim_id']>50]).mean(0)

# let's find the electrodes that distinguish faces from houses
from matplotlib import pyplot as plt

plt.figure(figsize=(20,10))
for j in range(50):
  ax = plt.subplot(5,10,j+1)
  plt.plot(trange, V_house[:,j])
  plt.plot(trange, V_face[:,j])
  plt.title('ch%d'%j)
  plt.xticks([-200, 0, 200])
  plt.ylim([0, 4])

# let's look at all the face trials for electrode 46 that has a good response to faces
# we will sort trials by stimulus id (1-50 is houses, 51-100 is faces)
plt.subplot(1,3,1)
isort = np.argsort(dat1['stim_id'])
plt.imshow(V_epochs[isort,:,46].astype('float32'), aspect='auto', vmax=7, vmin = 0, cmap = 'magma')
plt.colorbar()

# Electrode 43 seems to respond to houses
isort = np.argsort(dat1['stim_id'])
plt.subplot(1,3,1)
plt.imshow(V_epochs[isort,:,43].astype('float32'), aspect='auto', vmax=10, vmin = 0, cmap = 'magma')
plt.colorbar()

"""# FUNCTION"""

#@title broadband_power(dat)
# quick way to get broadband power in time-varying windows

def broadband_power(dat):
  from scipy import signal

  V = dat['V'].astype('float32')

  b, a = signal.butter(3, [50], btype = 'high', fs=1000)
  V = signal.filtfilt(b,a,V,0)
  V = np.abs(V)**2
  b, a = signal.butter(3, [10], btype = 'low', fs=1000)
  V = signal.filtfilt(b,a,V,0)

  V = V/V.mean(0)

  nt, nchan = V.shape
  nstim = len(dat['t_on'])

  trange = np.arange(-200, 400)
  ts = dat['t_on'][:,np.newaxis] + trange
  V_epochs = np.reshape(V[ts, :], (nstim, 600, nchan))
  return V_epochs

#@title select_channel(dat,V_epochs_dat,noiselevel)
# Function: select best channel
# Logistic regression
# noiselevel: input the highest noise level you want
# data1 set noiselevel=0


def select_channel(dat,V_epochs_dat,noiselevel):

  import itertools
  from scipy import signal
  from sklearn.preprocessing import StandardScaler
  from sklearn.decomposition import PCA
  from sklearn.linear_model import LogisticRegression
  from sklearn.model_selection import KFold
  from sklearn.model_selection import cross_val_score
  from sklearn.model_selection import train_test_split
  from sklearn.metrics import confusion_matrix, classification_report

  '''
  # quick way to get broadband power in time-varying windows (all trials together)
  V = dat['V'].astype('float32')
  b, a = signal.butter(3, [50], btype = 'high', fs=1000)
  V = signal.filtfilt(b,a,V,0)
  V = np.abs(V)**2
  b, a = signal.butter(3, [10], btype = 'low', fs=1000)
  V = signal.filtfilt(b,a,V,0)
  V = V/V.mean(0)
  nt, nchan = V.shape
  nstim = len(dat['t_on'])
  trange = np.arange(-200, 400)
  ts = dat['t_on'][:,np.newaxis] + trange
  V_epochs_dat = np.reshape(V[ts, :], (nstim, 600, nchan))
  '''

  # define list
  house=[]
  face=[]
  channels=[]
  channelnum=dat['locs'].shape[0]


  # noise data
  if 'stim_noise' in dat.keys():
    
    trials= np.squeeze(dat['stim_cat'])
    noise= np.squeeze(dat['stim_noise'])

    '''
    for j in range(channelnum): # choose all noise level
      for i in range(0,100,5): 
        house.append(V_epochs_dat[(trials == 1) & (noise == i)][:15,300:500,j])
        face.append(V_epochs_dat[(trials==2) & (noise == i)][:15,300:500,j])
    for k in range(channelnum):
      channels.append(np.vstack((np.squeeze(list(itertools.chain.from_iterable(house[k*20:20+(20*k)]))),(np.squeeze(list(itertools.chain.from_iterable(face[k*20:20+(20*k)])))))))
    channel_output= np.hstack((np.zeros(300),np.ones(300)))
    '''

    noiselevelrange=noiselevel+5
    outputsize=noiselevelrange*3
    inputlength=int(noiselevelrange/5)
    for j in range(channelnum): 
      for i in range(0,noiselevelrange,5):
        house.append(V_epochs_dat[(trials == 1) & (noise == i)][:15,300:500,j])
        face.append(V_epochs_dat[(trials==2) & (noise == i)][:15,300:500,j])
    for k in range(channelnum):
      channels.append(np.vstack((np.squeeze(list(itertools.chain.from_iterable(house[k*inputlength:inputlength+(inputlength*k)]))),(np.squeeze(list(itertools.chain.from_iterable(face[k*inputlength:inputlength+(inputlength*k)])))))))
    channel_output= np.hstack((np.zeros(outputsize),np.ones(outputsize)))


    Accuracy_total = np.zeros((5,channelnum))     # setting accuracy

    ## Logistic Regression Model ##
    for j in range (5):
      a = [1,2,3,4,5]     # 5 time cross validation
      for i in range(channelnum):   # iterate over 50 channels
        tr_X,tst_X,tr_Y,tst_Y= train_test_split(channels[i],channel_output,test_size= 0.2,shuffle= True,random_state= a[j])
        scaling= StandardScaler()
        tr_X= scaling.fit_transform(tr_X) 
        tst_X= scaling.transform(tst_X)
        #Principal Component Analysis
        pca= PCA(n_components= 0.9) # to capture 90 % varinace int he data
        tr_X= pca.fit_transform(tr_X)
        tst_X= pca.transform(tst_X) 
        var= pca.explained_variance_ratio_ * 100
        logreg = LogisticRegression(max_iter = 1000)
        logreg.fit(tr_X,tr_Y) # trained on the total dataset
        Yy_pred= logreg.predict(tst_X) # prediction will be done on the scaled test set from previous block
        Accuracy_total[j,i]= logreg.score(tst_X,tst_Y)
    Accuracy = Accuracy_total.mean(0)
    #plt.plot(Accuracy)
    channelNo = np.argmax(Accuracy)
    HighestAccuracy = Accuracy[channelNo]
    #print('Select Channel:',channelNo)
    #print('Highest Accuracy:',Accuracy[channelNo])

  # without noise data#
  else:

    for j in range(channelnum):
      channels.append(V_epochs_dat[:,300:500,j])
    channel_output_dat = np.zeros(channels[1].shape[0])
    channel_output_dat[dat["stim_id"]>50] = 1

    Accuracy = []
    ## Logistic Regression Model ##
    for i in range(channelnum):

      tr_x,tst_x,tr_y,tst_y = train_test_split(channels[i], channel_output_dat, test_size = 0.2, random_state = 1)
      scaling = StandardScaler()
      tr_x = scaling.fit_transform(tr_x) 
      tst_x = scaling.transform(tst_x)
      #Principal Component Analysis
      pca = PCA(n_components = 0.9) # to capture 90 % varinace int he data
      tr_x = pca.fit_transform(tr_x)
      tst_x = pca.transform(tst_x) 
      var = pca.explained_variance_ratio_ * 100
      logreg = LogisticRegression()
      logreg.fit(tr_x,tr_y) # trained on the total dataset
      Yy_pred= logreg.predict(tst_x) # prediction will be done on the scaled test set from previous block
      Accuracy.append(logreg.score(tst_x,tst_y))

    #plt.plot(Accuracy)
    channelNo = np.argmax(Accuracy)
    HighestAccuracy = Accuracy[channelNo]
    #print('Select Channel:',channelNo)
    #print('Highest Accuracy:',Accuracy[channelNo])

  return channelNo,HighestAccuracy,Accuracy

#@title select_channel_NB(dat,V_epochs_dat,noiselevel)
# Function: select best channel
# Logistic regression
# noiselevel: input the highest noise level you want
# data1 set noiselevel=0


def select_channel_NB(dat,V_epochs_dat,noiselevel):

  import itertools
  from scipy import signal
  from sklearn.preprocessing import StandardScaler
  from sklearn.decomposition import PCA
  from sklearn.linear_model import LogisticRegression
  from sklearn.model_selection import KFold
  from sklearn.model_selection import cross_val_score
  from sklearn.model_selection import train_test_split
  from sklearn.metrics import confusion_matrix, classification_report

  '''
  # quick way to get broadband power in time-varying windows (all trials together)
  V = dat['V'].astype('float32')
  b, a = signal.butter(3, [50], btype = 'high', fs=1000)
  V = signal.filtfilt(b,a,V,0)
  V = np.abs(V)**2
  b, a = signal.butter(3, [10], btype = 'low', fs=1000)
  V = signal.filtfilt(b,a,V,0)
  V = V/V.mean(0)
  nt, nchan = V.shape
  nstim = len(dat['t_on'])
  trange = np.arange(-200, 400)
  ts = dat['t_on'][:,np.newaxis] + trange
  V_epochs_dat = np.reshape(V[ts, :], (nstim, 600, nchan))
  '''

  # define list
  house=[]
  face=[]
  channels=[]
  channelnum=dat['locs'].shape[0]


  # noise data
  if 'stim_noise' in dat.keys():
    
    trials= np.squeeze(dat['stim_cat'])
    noise= np.squeeze(dat['stim_noise'])

    '''
    for j in range(channelnum): # choose all noise level
      for i in range(0,100,5): 
        house.append(V_epochs_dat[(trials == 1) & (noise == i)][:15,300:500,j])
        face.append(V_epochs_dat[(trials==2) & (noise == i)][:15,300:500,j])
    for k in range(channelnum):
      channels.append(np.vstack((np.squeeze(list(itertools.chain.from_iterable(house[k*20:20+(20*k)]))),(np.squeeze(list(itertools.chain.from_iterable(face[k*20:20+(20*k)])))))))
    channel_output= np.hstack((np.zeros(300),np.ones(300)))
    '''

    noiselevelrange=noiselevel+5
    outputsize=noiselevelrange*3
    inputlength=int(noiselevelrange/5)
    for j in range(channelnum): 
      for i in range(0,noiselevelrange,5):
        house.append(V_epochs_dat[(trials == 1) & (noise == i)][:15,300:500,j])
        face.append(V_epochs_dat[(trials==2) & (noise == i)][:15,300:500,j])
    for k in range(channelnum):
      channels.append(np.vstack((np.squeeze(list(itertools.chain.from_iterable(house[k*inputlength:inputlength+(inputlength*k)]))),(np.squeeze(list(itertools.chain.from_iterable(face[k*inputlength:inputlength+(inputlength*k)])))))))
    channel_output= np.hstack((np.zeros(outputsize),np.ones(outputsize)))


    Accuracy_total = np.zeros((5,channelnum))     # setting accuracy

    ## Logistic Regression Model ##
    for j in range (5):
      a = [1,2,3,4,5]     # 5 time cross validation
      for i in range(channelnum):   # iterate over 50 channels
        tr_X,tst_X,tr_Y,tst_Y= train_test_split(channels[i],channel_output,test_size= 0.2,shuffle= True,random_state= a[j])
        scaling= StandardScaler()
        tr_X= scaling.fit_transform(tr_X) 
        tst_X= scaling.transform(tst_X)
        #Principal Component Analysis
        pca= PCA(n_components= 0.9) # to capture 90 % varinace int he data
        tr_X= pca.fit_transform(tr_X)
        tst_X= pca.transform(tst_X) 
        var= pca.explained_variance_ratio_ * 100
        logreg = LogisticRegression(max_iter = 1000)
        logreg.fit(tr_X,tr_Y) # trained on the total dataset
        Yy_pred= logreg.predict(tst_X) # prediction will be done on the scaled test set from previous block
        Accuracy_total[j,i]= logreg.score(tst_X,tst_Y)
    Accuracy = Accuracy_total.mean(0)
    #plt.plot(Accuracy)
    channelNo = np.argmax(Accuracy)
    HighestAccuracy = Accuracy[channelNo]
    #print('Select Channel:',channelNo)
    #print('Highest Accuracy:',Accuracy[channelNo])

  # without noise data#
  else:

    for j in range(channelnum):
      channels.append(V_epochs_dat[:,300:500,j])
    channel_output_dat = np.zeros(channels[1].shape[0])
    channel_output_dat[dat["stim_id"]>50] = 1

    Accuracy = []
    ## Logistic Regression Model ##
    for i in range(channelnum):

      tr_x,tst_x,tr_y,tst_y = train_test_split(channels[i], channel_output_dat, test_size = 0.2, random_state = 1)
      scaling = StandardScaler()
      tr_x = scaling.fit_transform(tr_x) 
      tst_x = scaling.transform(tst_x)
      #Principal Component Analysis
      pca = PCA(n_components = 0.9) # to capture 90 % varinace int he data
      tr_x = pca.fit_transform(tr_x)
      tst_x = pca.transform(tst_x) 
      var = pca.explained_variance_ratio_ * 100
      #logreg = LogisticRegression()
      #logreg.fit(tr_x,tr_y) # trained on the total dataset
      #Yy_pred= logreg.predict(tst_x) # prediction will be done on the scaled test set from previous block
      #Accuracy.append(logreg.score(tst_x,tst_y))

      from sklearn import datasets
      from sklearn import metrics
      from sklearn.model_selection import train_test_split
      from sklearn.metrics import accuracy_score
      from sklearn.naive_bayes import GaussianNB
      nv = GaussianNB() # create a classifier
      nv.fit(tr_x,tr_y) # fitting the data
      y_pred = nv.predict(tst_x) # store the prediction data
      #print("___Naive Bayes Model___")
      #print("   Accuracy :",accuracy_score(y_test,y_pred)) # calculate the accuracy
      #print("   Precision :", metrics.precision_score(y_test,y_pred))
      #print("   Recall :" , metrics.recall_score(y_test,y_pred))
      #Accuracy = accuracy_score(y_test,y_pred)
      #Precision = metrics.precision_score(y_test,y_pred)
      #Recall = metrics.recall_score(y_test,y_pred)
      #return Accuracy,Precision,Recall
      Accuracy.append(accuracy_score(tst_y,y_pred))

    #plt.plot(Accuracy)
    channelNo = np.argmax(Accuracy)
    HighestAccuracy = Accuracy[channelNo]
    #print('Select Channel:',channelNo)
    #print('Highest Accuracy:',Accuracy[channelNo])

  return channelNo,HighestAccuracy,Accuracy

#@title select_channel_SVM(dat,V_epochs_dat,noiselevel)
# Function: select best channel
# Logistic regression
# noiselevel: input the highest noise level you want
# data1 set noiselevel=0


def select_channel_SVM(dat,V_epochs_dat,noiselevel):

  import itertools
  from scipy import signal
  from sklearn.preprocessing import StandardScaler
  from sklearn.decomposition import PCA
  from sklearn.linear_model import LogisticRegression
  from sklearn.model_selection import KFold
  from sklearn.model_selection import cross_val_score
  from sklearn.model_selection import train_test_split
  from sklearn.metrics import confusion_matrix, classification_report

  '''
  # quick way to get broadband power in time-varying windows (all trials together)
  V = dat['V'].astype('float32')
  b, a = signal.butter(3, [50], btype = 'high', fs=1000)
  V = signal.filtfilt(b,a,V,0)
  V = np.abs(V)**2
  b, a = signal.butter(3, [10], btype = 'low', fs=1000)
  V = signal.filtfilt(b,a,V,0)
  V = V/V.mean(0)
  nt, nchan = V.shape
  nstim = len(dat['t_on'])
  trange = np.arange(-200, 400)
  ts = dat['t_on'][:,np.newaxis] + trange
  V_epochs_dat = np.reshape(V[ts, :], (nstim, 600, nchan))
  '''

  # define list
  house=[]
  face=[]
  channels=[]
  channelnum=dat['locs'].shape[0]


  # noise data
  if 'stim_noise' in dat.keys():
    
    trials= np.squeeze(dat['stim_cat'])
    noise= np.squeeze(dat['stim_noise'])

    '''
    for j in range(channelnum): # choose all noise level
      for i in range(0,100,5): 
        house.append(V_epochs_dat[(trials == 1) & (noise == i)][:15,300:500,j])
        face.append(V_epochs_dat[(trials==2) & (noise == i)][:15,300:500,j])
    for k in range(channelnum):
      channels.append(np.vstack((np.squeeze(list(itertools.chain.from_iterable(house[k*20:20+(20*k)]))),(np.squeeze(list(itertools.chain.from_iterable(face[k*20:20+(20*k)])))))))
    channel_output= np.hstack((np.zeros(300),np.ones(300)))
    '''

    noiselevelrange=noiselevel+5
    outputsize=noiselevelrange*3
    inputlength=int(noiselevelrange/5)
    for j in range(channelnum): 
      for i in range(0,noiselevelrange,5):
        house.append(V_epochs_dat[(trials == 1) & (noise == i)][:15,300:500,j])
        face.append(V_epochs_dat[(trials==2) & (noise == i)][:15,300:500,j])
    for k in range(channelnum):
      channels.append(np.vstack((np.squeeze(list(itertools.chain.from_iterable(house[k*inputlength:inputlength+(inputlength*k)]))),(np.squeeze(list(itertools.chain.from_iterable(face[k*inputlength:inputlength+(inputlength*k)])))))))
    channel_output= np.hstack((np.zeros(outputsize),np.ones(outputsize)))


    Accuracy_total = np.zeros((5,channelnum))     # setting accuracy

    ## Logistic Regression Model ##
    for j in range (5):
      a = [1,2,3,4,5]     # 5 time cross validation
      for i in range(channelnum):   # iterate over 50 channels
        tr_X,tst_X,tr_Y,tst_Y= train_test_split(channels[i],channel_output,test_size= 0.2,shuffle= True,random_state= a[j])
        scaling= StandardScaler()
        tr_X= scaling.fit_transform(tr_X) 
        tst_X= scaling.transform(tst_X)
        #Principal Component Analysis
        pca= PCA(n_components= 0.9) # to capture 90 % varinace int he data
        tr_X= pca.fit_transform(tr_X)
        tst_X= pca.transform(tst_X) 
        var= pca.explained_variance_ratio_ * 100
        logreg = LogisticRegression(max_iter = 1000)
        logreg.fit(tr_X,tr_Y) # trained on the total dataset
        Yy_pred= logreg.predict(tst_X) # prediction will be done on the scaled test set from previous block
        Accuracy_total[j,i]= logreg.score(tst_X,tst_Y)
    Accuracy = Accuracy_total.mean(0)
    #plt.plot(Accuracy)
    channelNo = np.argmax(Accuracy)
    HighestAccuracy = Accuracy[channelNo]
    #print('Select Channel:',channelNo)
    #print('Highest Accuracy:',Accuracy[channelNo])

  # without noise data#
  else:

    for j in range(channelnum):
      channels.append(V_epochs_dat[:,300:500,j])
    channel_output_dat = np.zeros(channels[1].shape[0])
    channel_output_dat[dat["stim_id"]>50] = 1

    Accuracy = []
    ## Logistic Regression Model ##
    for i in range(channelnum):

      tr_x,tst_x,tr_y,tst_y = train_test_split(channels[i], channel_output_dat, test_size = 0.2, random_state = 1)
      scaling = StandardScaler()
      tr_x = scaling.fit_transform(tr_x) 
      tst_x = scaling.transform(tst_x)
      #Principal Component Analysis
      pca = PCA(n_components = 0.9) # to capture 90 % varinace int he data
      tr_x = pca.fit_transform(tr_x)
      tst_x = pca.transform(tst_x) 
      var = pca.explained_variance_ratio_ * 100

      from sklearn import svm
      from sklearn import metrics
      cls = svm.SVC(kernel="linear")
      cls.fit(tr_x,tr_y)
      pred = cls.predict(tst_x)
      #print("___SVM Classifier___")
      #print("   Acuracy:", metrics.accuracy_score(y_test,pred))
      #print("   Precision ", metrics.precision_score(y_test,pred))
      #print("   Recall" , metrics.recall_score(y_test,pred))
      #print(metrics.classification_report(y_test,pred))
      #Accuracy = metrics.accuracy_score(tst_y,pred)
      #Precision = metrics.precision_score(tst_y,pred)
      #Recall = metrics.recall_score(tst_y,pred)

      #return Accuracy,Precision,Recall
      #logreg = LogisticRegression()
      #logreg.fit(tr_x,tr_y) # trained on the total dataset
      #Yy_pred= logreg.predict(tst_x) # prediction will be done on the scaled test set from previous block
      Accuracy.append(metrics.accuracy_score(tst_y,pred))

      #plt.plot(Accuracy)
      channelNo = np.argmax(Accuracy)
      HighestAccuracy = Accuracy[channelNo]
      #print('Select Channel:',channelNo)
      #print('Highest Accuracy:',Accuracy[channelNo])

  return channelNo,HighestAccuracy,Accuracy

#@title generate_data(dat,V_epochs,channelNo,noiselevel)
# Function: extract input and output data for certain channel
# noiselevel: input the highest noise level you want for data2
# data1 set noiselevel=0

def generate_data(dat,V_epochs,channelNo,noiselevel):

  '''
  from scipy import signal
  V = dat['V'].astype('float32')
  b, a = signal.butter(3, [50], btype = 'high', fs=1000)
  V = signal.filtfilt(b,a,V,0)
  V = np.abs(V)**2
  b, a = signal.butter(3, [10], btype = 'low', fs=1000)
  V = signal.filtfilt(b,a,V,0)
  V = V/V.mean(0)
  nt, nchan = V.shape
  nstim = len(dat['t_on'])
  trange = np.arange(-200, 400)
  ts = dat['t_on'][:,np.newaxis] + trange
  V_epochs = np.reshape(V[ts, :], (nstim, 600, nchan))
  '''

  # noise data
  if 'stim_noise' in dat.keys():

    trials = np.squeeze(dat['stim_cat'])
    noise = np.squeeze(dat['stim_noise'])

    import itertools
    house = []
    face = []
    input = []
    noiselevelrange=noiselevel+5
    outputsize=noiselevelrange*3
    for i in range(0,noiselevelrange,5): 
      house.append(V_epochs[(trials == 1) & (noise == i)][:15,300:500,channelNo])
      face.append(V_epochs[(trials==2) & (noise == i)][:15,300:500,channelNo])
    input.append(np.vstack((np.squeeze(list(itertools.chain.from_iterable(house))),(np.squeeze(list(itertools.chain.from_iterable(face)))))))
    input = input[0]
    output= np.hstack((np.zeros(outputsize),np.ones(outputsize)))

  else:
    input = V_epochs[:,300:500,channelNo]
    output = np.zeros(input.shape[0])
    output[dat["stim_id"]>50] = 1
  return input, output

#@title combine_data(dat1,V_epochs1,channelNo1,dat2,V_epochs2,channelNo2)
# combine data1 & data 2 with all noise levels

def combine_data(dat1,V_epochs1,channelNo1,dat2,V_epochs2,channelNo2):

  import itertools
  no_noisy_input = V_epochs1[:,300:500,channelNo1]
  no_noisy_output = np.zeros(no_noisy_input.shape[0])
  no_noisy_output[dat1["stim_id"]>50] = 1

  trials= np.squeeze(dat2['stim_cat'])
  noise= np.squeeze(dat2['stim_noise'])
  house=[]
  face=[]
  for i in range(0,100,5):
    house.append(V_epochs2[(trials == 1) & (noise == i)][:15,300:500,channelNo2])
    face.append(V_epochs2[(trials==2) & (noise == i)][:15,300:500,channelNo2])

  # noise input & output
  house_noise_0 = np.squeeze(list(itertools.chain.from_iterable(house[0:5]))) # noise 0-20
  house_noise_1 = np.squeeze(list(itertools.chain.from_iterable(house[5:10]))) # nosie 25-45
  house_noise_2 = np.squeeze(list(itertools.chain.from_iterable(house[10:15]))) # noise 50-70
  house_noise_3 = np.squeeze(list(itertools.chain.from_iterable(house[15:20]))) # noise 75-95
  face_noise_0 = np.squeeze(list(itertools.chain.from_iterable(face[0:5]))) # noise 0-20
  face_noise_1 = np.squeeze(list(itertools.chain.from_iterable(face[5:10]))) # nosie 25-45
  face_noise_2 = np.squeeze(list(itertools.chain.from_iterable(face[10:15]))) # noise 50-70
  face_noise_3 = np.squeeze(list(itertools.chain.from_iterable(face[15:20]))) # noise 75-95

  noisy_input = np.concatenate((house_noise_0,face_noise_0,house_noise_1,face_noise_1,house_noise_2,face_noise_2,house_noise_3,face_noise_3))
  noisy_output = np.concatenate((np.zeros(75),np.ones(75),np.zeros(75),np.ones(75),np.zeros(75),np.ones(75),np.zeros(75),np.ones(75)))
 
  input = np.concatenate((no_noisy_input,noisy_input))
  output = np.concatenate((no_noisy_output,noisy_output))

  return input,output,noisy_input

#@title classify_noise(dat2,V_epochs2,channelNo2)
def classify_noise(dat2,V_epochs2,channelNo2):
  import itertools

  trials= np.squeeze(dat2['stim_cat'])
  noise= np.squeeze(dat2['stim_noise'])
  house=[]
  face=[]
  for i in range(0,100,5):
    house.append(V_epochs2[(trials == 1) & (noise == i)][:15,300:500,channelNo2])
    face.append(V_epochs2[(trials==2) & (noise == i)][:15,300:500,channelNo2])

  # noise input & output
  house_noise_0 = np.squeeze(list(itertools.chain.from_iterable(house[0:5]))) # noise 0-20
  house_noise_1 = np.squeeze(list(itertools.chain.from_iterable(house[5:10]))) # nosie 25-45
  house_noise_2 = np.squeeze(list(itertools.chain.from_iterable(house[10:15]))) # noise 50-70
  house_noise_3 = np.squeeze(list(itertools.chain.from_iterable(house[15:20]))) # noise 75-95
  face_noise_0 = np.squeeze(list(itertools.chain.from_iterable(face[0:5]))) # noise 0-20
  face_noise_1 = np.squeeze(list(itertools.chain.from_iterable(face[5:10]))) # nosie 25-45
  face_noise_2 = np.squeeze(list(itertools.chain.from_iterable(face[10:15]))) # noise 50-70
  face_noise_3 = np.squeeze(list(itertools.chain.from_iterable(face[15:20]))) # noise 75-95

  noisy_input = np.concatenate((house_noise_0,face_noise_0,house_noise_1,face_noise_1,house_noise_2,face_noise_2,house_noise_3,face_noise_3))
  noisy_output = np.concatenate((np.zeros(75),np.ones(75),np.zeros(75),np.ones(75),np.zeros(75),np.ones(75),np.zeros(75),np.ones(75)))

  return noisy_input,noisy_output

#@title select_noise(dat2,V_epochs2,channelNo2)
def select_noise(dat2,V_epochs2,channelNo2):
  import itertools

  trials= np.squeeze(dat2['stim_cat'])
  noise= np.squeeze(dat2['stim_noise'])
  house=[]
  face=[]
  for i in range(0,100,5):
    house.append(V_epochs2[(trials == 1) & (noise == i)][:15,300:500,channelNo2])
    face.append(V_epochs2[(trials==2) & (noise == i)][:15,300:500,channelNo2])

  # noise input & output
  house_noise_0 = np.squeeze(list(itertools.chain.from_iterable(house[0:5]))) # noise 0-20
  house_noise_1 = np.squeeze(list(itertools.chain.from_iterable(house[5:10]))) # nosie 25-45
  #house_noise_2 = np.squeeze(list(itertools.chain.from_iterable(house[10:15]))) # noise 50-70
  #house_noise_3 = np.squeeze(list(itertools.chain.from_iterable(house[15:20]))) # noise 75-95
  face_noise_0 = np.squeeze(list(itertools.chain.from_iterable(face[0:5]))) # noise 0-20
  face_noise_1 = np.squeeze(list(itertools.chain.from_iterable(face[5:10]))) # nosie 25-45
  #face_noise_2 = np.squeeze(list(itertools.chain.from_iterable(face[10:15]))) # noise 50-70
  #face_noise_3 = np.squeeze(list(itertools.chain.from_iterable(face[15:20]))) # noise 75-95

  noisy_input = np.concatenate((house_noise_0,face_noise_0,house_noise_1,face_noise_1))
  noisy_output = np.concatenate((np.zeros(75),np.ones(75),np.zeros(75),np.ones(75)))

  return noisy_input,noisy_output

#@title add_noise_feature(dat2,input,output,noisy_input):
# Function: extract input and output data for certain channel
# both data 1&2
# with noise as feature


def add_noise_feature(dat2,input,output,noisy_input):

  import itertools
  #Noise feature
  #trials= np.squeeze(dat2['stim_cat'])
  #noise= np.squeeze(dat2['stim_noise'])

  #print(noise[X_train].shape)
  
  idx = np.arange(len(noisy_input))
  np.random.seed(0)
  np.random.shuffle(idx)
  train_idx, test_idx = idx[:480],idx[480:]
  train_X,train_Y= input[np.array([train_idx])], output[np.array([train_idx])]
  np.squeeze(train_X).shape

  #noise= dat2['stim_noise']
  merged_noise = list(itertools.chain.from_iterable(noisy_input))
  merged_noise= [item for item in merged_noise if item != 100]
  noise_train= np.array(merged_noise)[train_idx]
  noise_test= np.array(merged_noise)[test_idx]
  train_X= list(itertools.chain.from_iterable(train_X))
  train_Y= list(itertools.chain.from_iterable(train_Y))
  test_X= np.squeeze(input[np.array([test_idx])])
  test_Y= np.squeeze(output[np.array([test_idx])])

  #Principal Component Analysis
  from sklearn.decomposition import PCA
  pca= PCA(n_components= 0.9) # to capture 90 % varinace int he data
  train_X= pca.fit_transform(train_X)
  test_X= pca.transform(test_X) 

  var= pca.explained_variance_ratio_ * 100


  new_train=[]
  for i in range(len(train_X)):
    new_train.append(np.append(train_X[i],noise_train[i]))

  train_X= np.array(new_train)
  new_test=[]
  for i in range(len(test_X)):
    new_test.append(np.append(test_X[i],noise_test[i]))
  test_X= np.array(new_test)

  # standardisation
  #feature scaling (standardisation)
  from sklearn.preprocessing import StandardScaler
    #train_test(all_input,all_output)
  scaling= StandardScaler()
  train_X= scaling.fit_transform(train_X) 
  test_X= scaling.transform(test_X) # scaled on test set from the first experiment


  return train_X,test_X,train_Y,test_Y

#@title add_noise_feature45(dat2,input,output,noisy_input):
# Function: extract input and output data for certain channel
# both data 1&2
# with noise as feature
# noise level 0-45


def add_noise_feature45(dat2,input,output,noisy_input):

  import itertools
  #Noise feature
  #trials= np.squeeze(dat2['stim_cat'])
  #noise= np.squeeze(dat2['stim_noise'])

  #print(noise[X_train].shape)
  
  idx = np.arange(len(noisy_input))
  np.random.seed(0)
  np.random.shuffle(idx)
  train_idx, test_idx = idx[:240],idx[240:]
  train_X,train_Y= input[np.array([train_idx])], output[np.array([train_idx])]
  np.squeeze(train_X).shape

  #noise= dat2['stim_noise']
  merged_noise = list(itertools.chain.from_iterable(noisy_input))
  merged_noise= [item for item in merged_noise if item != 100]
  noise_train= np.array(merged_noise)[train_idx]
  noise_test= np.array(merged_noise)[test_idx]
  train_X= list(itertools.chain.from_iterable(train_X))
  train_Y= list(itertools.chain.from_iterable(train_Y))
  test_X= np.squeeze(input[np.array([test_idx])])
  test_Y= np.squeeze(output[np.array([test_idx])])

  #Principal Component Analysis
  from sklearn.decomposition import PCA
  pca= PCA(n_components= 0.9) # to capture 90 % varinace int he data
  train_X= pca.fit_transform(train_X)
  test_X= pca.transform(test_X) 

  var= pca.explained_variance_ratio_ * 100


  new_train=[]
  for i in range(len(train_X)):
    new_train.append(np.append(train_X[i],noise_train[i]))

  train_X= np.array(new_train)
  new_test=[]
  for i in range(len(test_X)):
    new_test.append(np.append(test_X[i],noise_test[i]))
  test_X= np.array(new_test)

  # standardisation
  #feature scaling (standardisation)
  from sklearn.preprocessing import StandardScaler
    #train_test(all_input,all_output)
  scaling= StandardScaler()
  train_X= scaling.fit_transform(train_X) 
  test_X= scaling.transform(test_X) # scaled on test set from the first experiment


  return train_X,test_X,train_Y,test_Y

#@title DemensionReduction(input,output)
# Function: Demension Reduction
def DemensionReduction(input,output):

  # training and test set
  import random
  from sklearn.model_selection import train_test_split
  x_train,x_test,y_train,y_test= train_test_split(input,output,test_size= 0.2,random_state= None)


  #feature scaling (standardisation)
  from sklearn.preprocessing import StandardScaler
  scaling= StandardScaler()
  x_train= scaling.fit_transform(x_train)
  x_test= scaling.transform(x_test)

  #Principal Component Analysis
  from sklearn.decomposition import PCA
  pca= PCA(n_components = 0.9) # to capture 90 % varinace int he data
  x_train = pca.fit_transform(x_train)
  x_test = pca.transform(x_test)
  var= pca.explained_variance_ratio_ * 100

  return x_train, x_test, y_train, y_test

#@title logistic_regression(x_train,x_test,y_train,y_test)
#Function: Training logistic regression model
def logistic_regression(x_train,x_test,y_train,y_test):
  from sklearn.linear_model import LogisticRegression
  from sklearn.model_selection import KFold
  from sklearn.model_selection import cross_val_score
  from sklearn import metrics
  from sklearn.metrics import confusion_matrix, classification_report
  from sklearn.metrics import accuracy_score
  #standardisation(x_train,x_test)
  logreg = LogisticRegression()
  logreg.fit(x_train, y_train)
  # Testing the logistic regression model 
  y_pred= logreg.predict(x_test)
  #print("___Logistic Regression___")
  #print("   Accuracy : ",logreg.score(x_test,y_test))
  #print("   Precision :", metrics.precision_score(y_test,y_pred))
  #print("   Recall :" , metrics.recall_score(y_test,y_pred))
  cv = KFold(n_splits=10, random_state=None)
  #print('   Avg Crossvalidation Score : ',(cross_val_score(logreg,x_train,y_train,cv=10)).mean(0))
  Accuracy = logreg.score(x_test,y_test)
  Precision = metrics.precision_score(y_test,y_pred)
  Recall = metrics.recall_score(y_test,y_pred)
  return Accuracy,Precision,Recall

#@title naive_bayes(x_train,x_test,y_train,y_test)
# Function: Naive Bayes
def naive_bayes(x_train,x_test,y_train,y_test):
  from sklearn import datasets
  from sklearn import metrics
  from sklearn.model_selection import train_test_split
  from sklearn.metrics import accuracy_score
  from sklearn.naive_bayes import GaussianNB
  nv = GaussianNB() # create a classifier
  nv.fit(x_train,y_train) # fitting the data
  y_pred = nv.predict(x_test) # store the prediction data
  #print("___Naive Bayes Model___")
  #print("   Accuracy :",accuracy_score(y_test,y_pred)) # calculate the accuracy
  #print("   Precision :", metrics.precision_score(y_test,y_pred))
  #print("   Recall :" , metrics.recall_score(y_test,y_pred))
  Accuracy = accuracy_score(y_test,y_pred)
  Precision = metrics.precision_score(y_test,y_pred)
  Recall = metrics.recall_score(y_test,y_pred)
  return Accuracy,Precision,Recall

#@title SVM_classify(x_train,x_test,y_train,y_test)
# Function: SVM
def SVM_classify(x_train,x_test,y_train,y_test):
  from sklearn import svm
  from sklearn import metrics
  cls = svm.SVC(kernel="linear")
  cls.fit(x_train,y_train)
  pred = cls.predict(x_test)
  #print("___SVM Classifier___")
  #print("   Acuracy:", metrics.accuracy_score(y_test,pred))
  #print("   Precision ", metrics.precision_score(y_test,pred))
  #print("   Recall" , metrics.recall_score(y_test,pred))
  #print(metrics.classification_report(y_test,pred))
  Accuracy = metrics.accuracy_score(y_test,pred)
  Precision = metrics.precision_score(y_test,pred)
  Recall = metrics.recall_score(y_test,pred)
  return Accuracy,Precision,Recall

"""# DATA SET 1"""

# Select best channel for data1 (without noise)
# Logistic Regression Model

#for i in range(7):
dat1 = alldat[1][0]
V_epochs = broadband_power(dat1)
print('Participant:',1)
[channelNo,HighestAccuracy,Accuracy] = select_channel(dat1,V_epochs,0)
print('Select Channel:',channelNo)
print('Highest Accuracy:',Accuracy[channelNo])
plt.plot(Accuracy,label= 1)
plt.legend()
plt.xlabel('Channel Number')
plt.ylabel('Accuracy')
plt.title('Participant 1')

# Select best channel for data1 (without noise)
# SVM Model

for i in range(7):
  dat1 = alldat[i][0]
  V_epochs = broadband_power(dat1)
  print('Participant:',i)
  [channelNo,HighestAccuracy,Accuracy] = select_channel_SVM(dat1,V_epochs,0)
  print('Select Channel:',channelNo)
  print('Highest Accuracy:',Accuracy[channelNo])
plt.plot(Accuracy,label= 1)
plt.legend()
plt.xlabel('Channel Number')
plt.ylabel('Accuracy')
plt.title('Participant 1')

# Select best channel for data1 (without noise)
# Naive Bayes Model

for i in range(7):
  dat1 = alldat[i][0]
  V_epochs = broadband_power(dat1)
  print('Participant:',i)
  [channelNo,HighestAccuracy,Accuracy] = select_channel_NB(dat1,V_epochs,0)
  print('Select Channel:',channelNo)
  print('Highest Accuracy:',Accuracy[channelNo])
plt.plot(Accuracy,label= 1)
plt.legend()
plt.xlabel('Channel Number')
plt.ylabel('Accuracy')
plt.title('Participant 1')

# Choose best channel and test each model for each participant
# Train set: data1
# Test set: data1

import sys
from prettytable import PrettyTable
#reload(sys)
#sys.setdefaultencoding('utf8')

table = PrettyTable(['Participant','Select Channel','Model','Accuracy','Precision','Recall'])
for i in range(7):
  dat1 = alldat[i][0]
  V_epochs = broadband_power(dat1)
  [channelNo,HighestAccuracy,Accuracy] = select_channel(dat1,V_epochs,0)
  [input,output] = generate_data(dat1,V_epochs,channelNo,0)
  [x_train, x_test, y_train, y_test] = DemensionReduction(input,output)
  [AccuracyLG,PrecisionLG,RecallLG] = logistic_regression(x_train,x_test,y_train,y_test) #Logistic Regression
  [AccuracyNB,PrecisionNB,RecallNB] = naive_bayes(x_train,x_test,y_train,y_test) # Naive Bayes
  [AccuracySVM,PrecisionSVM,RecallSVM] = SVM_classify(x_train,x_test,y_train,y_test) #SVM
  table.add_row([i,channelNo,'Logistic Regression',AccuracyLG,PrecisionLG,RecallLG])
  table.add_row([i,channelNo,'Naive Bayes',AccuracyNB,PrecisionNB,RecallNB])
  table.add_row([i,channelNo,'SVM',AccuracySVM,PrecisionSVM,RecallSVM])
  plt.plot([AccuracyLG,AccuracyNB,AccuracySVM])
print(table)

# Choose best channel and test each model for each participant
# Train set: data1
# Test set: data1

import sys
from prettytable import PrettyTable
#reload(sys)
#sys.setdefaultencoding('utf8')

table = PrettyTable(['Participant','Select Channel','Model','Accuracy','Precision','Recall'])
dat1 = alldat[1][0]
V_epochs = broadband_power(dat1)
[channelNo,HighestAccuracy,Accuracy] = select_channel(dat1,V_epochs,0)
[input,output] = generate_data(dat1,V_epochs,channelNo,0)
[x_train, x_test, y_train, y_test] = DemensionReduction(input,output)
[AccuracyLG,PrecisionLG,RecallLG] = logistic_regression(x_train,x_test,y_train,y_test) #Logistic Regression
[AccuracyNB,PrecisionNB,RecallNB] = naive_bayes(x_train,x_test,y_train,y_test) # Naive Bayes
[AccuracySVM,PrecisionSVM,RecallSVM] = SVM_classify(x_train,x_test,y_train,y_test) #SVM
table.add_row([1,channelNo,'Logistic Regression',AccuracyLG,PrecisionLG,RecallLG])
table.add_row([1,channelNo,'Naive Bayes',AccuracyNB,PrecisionNB,RecallNB])
table.add_row([1,channelNo,'SVM',AccuracySVM,PrecisionSVM,RecallSVM])
xticks= (['Logistic Regression','SVM','Naive Bayes'])
plt.plot(xticks,[AccuracyLG,AccuracySVM,AccuracyNB])

plt.title("Participant 1")
print(table)

"""# DATA SET 2"""

# Select best channel for data2 (with noise)
# Logistic Regression Model
# set noise level = 0-45%

noiselevel=45

for i in range(7):
  dat2 = alldat[i][1]
  V_epochs = broadband_power(dat2)
  print('Participant:',i)
  [channelNo,HighestAccuracy,Accuracy] = select_channel(dat2,V_epochs,noiselevel) #noiselevel 0-45%
  print('Select Channel:',channelNo)
  print('Highest Accuracy:',Accuracy[channelNo])
plt.plot(Accuracy,label= 1)
plt.legend()
plt.xlabel('Channel Number')
plt.ylabel('Accuracy')

# Choose best channel and test each model for each participant
# Train set: data2
# Test set: data2
# set noise level = 0-45%
noiselevel=45

import sys
from prettytable import PrettyTable
#reload(sys)
#sys.setdefaultencoding('utf8')

table = PrettyTable(['Participant','Select Channel','Model','Accuracy','Precision','Recall'])
for i in range(7):
  dat2 = alldat[i][1]
  V_epochs = broadband_power(dat2)
  [channelNo,HighestAccuracy,Accuracy] = select_channel(dat2,V_epochs,noiselevel)
  [input,output] = generate_data(dat2,V_epochs,channelNo,noiselevel) 
  [x_train, x_test, y_train, y_test] = DemensionReduction(input,output)
  [AccuracyLG,PrecisionLG,RecallLG] = logistic_regression(x_train,x_test,y_train,y_test) #Logistic Regression
  [AccuracyNB,PrecisionNB,RecallNB] = naive_bayes(x_train,x_test,y_train,y_test) # Naive Bayes
  [AccuracySVM,PrecisionSVM,RecallSVM] = SVM_classify(x_train,x_test,y_train,y_test) #SVM
  table.add_row([i,channelNo,'Logistic Regression',AccuracyLG,PrecisionLG,RecallLG])
  table.add_row([i,channelNo,'Naive Bayes',AccuracyNB,PrecisionNB,RecallNB])
  table.add_row([i,channelNo,'SVM',AccuracySVM,PrecisionSVM,RecallSVM])
print(table)

# Choose best channel and test each model for each participant
# Train set: data2
# Test set: data2
# set noise level = 0-45%
noiselevel=45

import sys
from prettytable import PrettyTable
#reload(sys)
#sys.setdefaultencoding('utf8')

table = PrettyTable(['Participant','Select Channel','Model','Accuracy','Precision','Recall'])
#for i in range(7):
dat2 = alldat[1][1]
V_epochs = broadband_power(dat2)
[channelNo,HighestAccuracy,Accuracy] = select_channel(dat2,V_epochs,noiselevel)
[input,output] = generate_data(dat2,V_epochs,channelNo,noiselevel) 
[x_train, x_test, y_train, y_test] = DemensionReduction(input,output)
[AccuracyLG,PrecisionLG,RecallLG] = logistic_regression(x_train,x_test,y_train,y_test) #Logistic Regression
[AccuracyNB,PrecisionNB,RecallNB] = naive_bayes(x_train,x_test,y_train,y_test) # Naive Bayes
[AccuracySVM,PrecisionSVM,RecallSVM] = SVM_classify(x_train,x_test,y_train,y_test) #SVM
table.add_row([1,channelNo,'Logistic Regression',AccuracyLG,PrecisionLG,RecallLG])
table.add_row([1,channelNo,'Naive Bayes',AccuracyNB,PrecisionNB,RecallNB])
table.add_row([1,channelNo,'SVM',AccuracySVM,PrecisionSVM,RecallSVM])
xticks= (['Logistic Regression','SVM','Naive Bayes'])
plt.plot(xticks,[AccuracyLG,AccuracySVM,AccuracyNB])
print(table)

# Choose best channel and test each model for each participant
# Train set: data2
# Test set: data2
# set noise level = 0-95%
noiselevel=95

import sys
from prettytable import PrettyTable
#reload(sys)
#sys.setdefaultencoding('utf8')

table = PrettyTable(['Participant','Select Channel','Model','Accuracy','Precision','Recall'])
for i in range(7):
  dat2 = alldat[i][1]
  V_epochs = broadband_power(dat2)
  [channelNo,HighestAccuracy,Accuracy] = select_channel(dat2,V_epochs,noiselevel)
  [input,output] = generate_data(dat2,V_epochs,channelNo,noiselevel) 
  [x_train, x_test, y_train, y_test] = DemensionReduction(input,output)
  [AccuracyLG,PrecisionLG,RecallLG] = logistic_regression(x_train,x_test,y_train,y_test) #Logistic Regression
  [AccuracyNB,PrecisionNB,RecallNB] = naive_bayes(x_train,x_test,y_train,y_test) # Naive Bayes
  [AccuracySVM,PrecisionSVM,RecallSVM] = SVM_classify(x_train,x_test,y_train,y_test) #SVM
  table.add_row([i,channelNo,'Logistic Regression',AccuracyLG,PrecisionLG,RecallLG])
  table.add_row([i,channelNo,'Naive Bayes',AccuracyNB,PrecisionNB,RecallNB])
  table.add_row([i,channelNo,'SVM',AccuracySVM,PrecisionSVM,RecallSVM])
print(table)

# Choose best channel and test each model for each participant
# Train set: data2
# Test set: data2
# set noise level = 0-95%

# divide noise level into different group

noiselevel=95

import sys
from prettytable import PrettyTable
#reload(sys)
#sys.setdefaultencoding('utf8')

table = PrettyTable(['Participant','Select Channel','Model','Accuracy','Precision','Recall'])
for i in range(7):
  dat2 = alldat[i][1]
  V_epochs = broadband_power(dat2)
  [channelNo,HighestAccuracy,Accuracy] = select_channel(dat2,V_epochs,noiselevel)
  [noise_input,noise_output] = classify_noise(dat2,V_epochs,channelNo)
  [x_train, x_test, y_train, y_test] = DemensionReduction(noise_input,noise_output)
  [AccuracyLG,PrecisionLG,RecallLG] = logistic_regression(x_train,x_test,y_train,y_test) #Logistic Regression
  [AccuracyNB,PrecisionNB,RecallNB] = naive_bayes(x_train,x_test,y_train,y_test) # Naive Bayes
  [AccuracySVM,PrecisionSVM,RecallSVM] = SVM_classify(x_train,x_test,y_train,y_test) #SVM
  table.add_row([i,channelNo,'Logistic Regression',AccuracyLG,PrecisionLG,RecallLG])
  table.add_row([i,channelNo,'Naive Bayes',AccuracyNB,PrecisionNB,RecallNB])
  table.add_row([i,channelNo,'SVM',AccuracySVM,PrecisionSVM,RecallSVM])
print(table)

# Choose best channel and test each model for each participant
# Set noise levels as features (0-95%)
# Train set: data 2
# Test set: data 2


import sys
from prettytable import PrettyTable
#reload(sys)
#sys.setdefaultencoding('utf8')

noiselevel=95

table = PrettyTable(['Participant','Select Channel','Model','Accuracy','Precision','Recall'])
for i in range(7):

  dat2 = alldat[i][1]
  V_epochs2 = broadband_power(dat2)
  [channelNo2,HighestAccuracy2,Accuracy2] = select_channel(dat2,V_epochs2,noiselevel)
  [noise_input, noise_output] = classify_noise(dat2,V_epochs2,channelNo2)
  [x_train, x_test, y_train, y_test] = add_noise_feature(dat2,noise_input, noise_output,noise_input)

  [AccuracyLG,PrecisionLG,RecallLG] = logistic_regression(x_train,x_test,y_train,y_test) #Logistic Regression
  [AccuracyNB,PrecisionNB,RecallNB] = naive_bayes(x_train,x_test,y_train,y_test) # Naive Bayes
  [AccuracySVM,PrecisionSVM,RecallSVM] = SVM_classify(x_train,x_test,y_train,y_test) #SVM
  table.add_row([i,channelNo2,'Logistic Regression',AccuracyLG,PrecisionLG,RecallLG])
  table.add_row([i,channelNo2,'Naive Bayes',AccuracyNB,PrecisionNB,RecallNB])
  table.add_row([i,channelNo2,'SVM',AccuracySVM,PrecisionSVM,RecallSVM])
print(table)

# Choose best channel and test each model for each participant
# Set noise levels as features (0-45%)
# Train set: data 2
# Test set: data 2


import sys
from prettytable import PrettyTable
#reload(sys)
#sys.setdefaultencoding('utf8')

noiselevel=45

table = PrettyTable(['Participant','Select Channel','Model','Accuracy','Precision','Recall'])
for i in range(7):

  dat2 = alldat[i][1]
  V_epochs2 = broadband_power(dat2)
  [channelNo2,HighestAccuracy2,Accuracy2] = select_channel(dat2,V_epochs2,noiselevel)
  [noise_input, noise_output] = select_noise(dat2,V_epochs2,channelNo2)
  [x_train, x_test, y_train, y_test] = add_noise_feature45(dat2,noise_input, noise_output,noise_input)

  [AccuracyLG,PrecisionLG,RecallLG] = logistic_regression(x_train,x_test,y_train,y_test) #Logistic Regression
  [AccuracyNB,PrecisionNB,RecallNB] = naive_bayes(x_train,x_test,y_train,y_test) # Naive Bayes
  [AccuracySVM,PrecisionSVM,RecallSVM] = SVM_classify(x_train,x_test,y_train,y_test) #SVM
  table.add_row([i,channelNo2,'Logistic Regression',AccuracyLG,PrecisionLG,RecallLG])
  table.add_row([i,channelNo2,'Naive Bayes',AccuracyNB,PrecisionNB,RecallNB])
  table.add_row([i,channelNo2,'SVM',AccuracySVM,PrecisionSVM,RecallSVM])
print(table)

"""# COMBINE DATASET"""

# Choose best channel and test each model for each participant
# Train set: data1&2
# Test set: data1&2
# set noise level = 0-95%

import sys
from prettytable import PrettyTable
#reload(sys)
#sys.setdefaultencoding('utf8')

noiselevel=95

table = PrettyTable(['Participant','Select Channel for data1','Select Channel for data2','Model','Accuracy','Precision','Recall'])
for i in range(7):
  dat1 = alldat[i][0]
  V_epochs1 = broadband_power(dat1)
  [channelNo1,HighestAccuracy1,Accuracy1] = select_channel(dat1,V_epochs1,0)
  #[input1,output1] = generate_data(dat1,V_epochs1,channelNo1,0)

  dat2 = alldat[i][1]
  V_epochs2 = broadband_power(dat2)
  [channelNo2,HighestAccuracy2,Accuracy2] = select_channel(dat2,V_epochs2,noiselevel)
  #[input2,output2] = generate_data(dat2,V_epochs2,channelNo2,noiselevel) 

  #input = np.concatenate((input1, input2))
  #output = np.concatenate((output1, output2))
  [input,output,_] = combine_data(dat1,V_epochs1,channelNo1,dat2,V_epochs2,channelNo2)

  [x_train, x_test, y_train, y_test] = DemensionReduction(input,output)
  [AccuracyLG,PrecisionLG,RecallLG] = logistic_regression(x_train,x_test,y_train,y_test) #Logistic Regression
  [AccuracyNB,PrecisionNB,RecallNB] = naive_bayes(x_train,x_test,y_train,y_test) # Naive Bayes
  [AccuracySVM,PrecisionSVM,RecallSVM] = SVM_classify(x_train,x_test,y_train,y_test) #SVM
  table.add_row([i,channelNo1,channelNo2,'Logistic Regression',AccuracyLG,PrecisionLG,RecallLG])
  table.add_row([i,channelNo1,channelNo2,'Naive Bayes',AccuracyNB,PrecisionNB,RecallNB])
  table.add_row([i,channelNo1,channelNo2,'SVM',AccuracySVM,PrecisionSVM,RecallSVM])
print(table)

# Choose best channel and test each model for each participant
# Set noise levels as features (0-95%)
# Train set: data 1&2
# Test set: data 1&2


import sys
from prettytable import PrettyTable
#reload(sys)
#sys.setdefaultencoding('utf8')

noiselevel=95

table = PrettyTable(['Participant','Select Channel for data1','Select Channel for data2','Model','Accuracy','Precision','Recall'])
for i in range(7):
  dat1 = alldat[i][0]
  V_epochs1 = broadband_power(dat1)
  [channelNo1,HighestAccuracy1,Accuracy1] = select_channel(dat1,V_epochs1,0)
  #[input1,output1] = generate_data(dat1,V_epochs1,channelNo1,0)

  dat2 = alldat[i][1]
  V_epochs2 = broadband_power(dat2)
  [channelNo2,HighestAccuracy2,Accuracy2] = select_channel(dat2,V_epochs2,noiselevel)
  #[input2,output2] = generate_data(dat2,V_epochs2,channelNo2,noiselevel) 

  #input = np.concatenate((input1, input2))
  #output = np.concatenate((output1, output2))
  [input,output,noisy_input] = combine_data(dat1,V_epochs1,channelNo1,dat2,V_epochs2,channelNo2)
  [x_train, x_test, y_train, y_test] = add_noise_feature(dat2,input,output,noisy_input)

  [AccuracyLG,PrecisionLG,RecallLG] = logistic_regression(x_train,x_test,y_train,y_test) #Logistic Regression
  [AccuracyNB,PrecisionNB,RecallNB] = naive_bayes(x_train,x_test,y_train,y_test) # Naive Bayes
  [AccuracySVM,PrecisionSVM,RecallSVM] = SVM_classify(x_train,x_test,y_train,y_test) #SVM
  table.add_row([i,channelNo1,channelNo2,'Logistic Regression',AccuracyLG,PrecisionLG,RecallLG])
  table.add_row([i,channelNo1,channelNo2,'Naive Bayes',AccuracyNB,PrecisionNB,RecallNB])
  table.add_row([i,channelNo1,channelNo2,'SVM',AccuracySVM,PrecisionSVM,RecallSVM])
print(table)

"""# PLOT - Amplitude across Noise"""

import itertools
#set noiselevel = 0-45%
noiselevel=45

dat2 = alldat[1][1]

V_epochs2 = broadband_power(dat2)
#[channelNo2,HighestAccuracy2,Accuracy2] = select_channel(dat2,V_epochs2,noiselevel)
channelNo2=int(46)

trials= np.squeeze(dat2['stim_cat'])
noise= np.squeeze(dat2['stim_noise'])
house=[]
face=[]
house_noise=[]
face_noise=[]
for i in range(0,100,5):
  house.append(V_epochs2[(trials == 1) & (noise == i)][:15,300:500,channelNo2])
  face.append(V_epochs2[(trials==2) & (noise == i)][:15,300:500,channelNo2])
house_noise.append(np.vstack((np.squeeze(list(itertools.chain.from_iterable(house))))))
face_noise.append(np.vstack((np.squeeze(np.squeeze(list(itertools.chain.from_iterable(face)))))))
house_noise = house_noise[0]
face_noise = face_noise[0]

plt.figure(figsize=(20,10))
for i in range(10):
  plt.subplot(2,1,1)
  plt.plot(house_noise[i][:200],label='%s Noise'%(i*5))
  plt.title('%s House Channel'%channelNo2)  
plt.legend()
plt.yticks([0,1,2,3,4,5])
plt.xlabel('Time')
plt.ylabel('Amplitude')
for i in range(10):
  plt.subplot(2,1,2)
  plt.plot(face_noise[i][:200],label='%s Noise'%(i*5))
  plt.title('%s Face Channel'%channelNo2) 
plt.legend()
plt.yticks([0,1,2,3,4,5])
plt.xlabel('Time')
plt.ylabel('Amplitude')